#!/usr/bin/env python3
"""
Test multiforecaster on real Metaculus questions with detailed logging.
"""
import asyncio
import os
import sys
from datetime import datetime
from pathlib import Path

# Set up environment
os.environ['GITHUB_ACTIONS'] = 'true'
os.environ['OPENAI_DISABLE_TRACE'] = 'true'

async def test_real_questions():
    """Test multiforecaster on real questions."""
    try:
        from fallback_llm import create_default_fallback_llm, create_research_fallback_llm, create_synthesis_fallback_llm, create_forecasting_fallback_llm
        from forecasting_tools import MetaculusApi, BinaryQuestion
        from forecasting_tools.helpers.metaculus_api import ApiFilter

        print("üéØ TESTING MULTIFORECASTER ON REAL METACULUS QUESTIONS")
        print("=" * 70)
        print("This will:")
        print("1. Find real open questions from available tournaments")
        print("2. Use detailed API call logging")
        print("3. Run complete multiforecaster process")
        print("4. Save all outputs to /outputs/ folder")

        # Get API keys
        api_key = os.getenv('OPENROUTER_API_KEY')
        metaculus_token = os.getenv('METACULUS_TOKEN')

        if not api_key:
            print("‚ùå OPENROUTER_API_KEY is not set!")
            return False

        if not metaculus_token:
            print("‚ùå METACULUS_TOKEN is not set!")
            return False

        print(f"‚úÖ API keys configured")

        # Create outputs directory
        output_dir = Path("outputs")
        output_dir.mkdir(exist_ok=True)

        # Step 1: Find real questions
        print(f"\nüìä STEP 1: FINDING REAL QUESTIONS")

        # Try different tournaments to find real questions
        tournaments_to_try = [
            MetaculusApi.CURRENT_AI_COMPETITION_ID if hasattr(MetaculusApi, 'CURRENT_AI_COMPETITION_ID') else None,
            MetaculusApi.CURRENT_MINIBENCH_ID if hasattr(MetaculusApi, 'CURRENT_MINIBENCH_ID') else None,
            "general",  # Try general questions
        ]

        real_questions = []
        tournament_used = None

        for tournament in tournaments_to_try:
            if tournament is None:
                continue

            print(f"üîç Checking tournament: {tournament}")

            try:
                if tournament == "general":
                    # Get general open questions
                    questions = await MetaculusApi.get_questions_matching_filter(
                        ApiFilter(allowed_statuses=["open"])
                    )
                else:
                    # Get tournament-specific questions
                    questions = await MetaculusApi.get_questions_matching_filter(
                        ApiFilter(
                            allowed_statuses=["open"],
                            allowed_tournaments=[tournament]
                        )
                    )

                print(f"‚úÖ Found {len(questions)} open questions for tournament '{tournament}'")

                if questions:
                    real_questions = questions
                    tournament_used = tournament
                    break

            except Exception as e:
                print(f"‚ùå Failed to get questions for '{tournament}': {str(e)}")
                continue

        if not real_questions:
            print("‚ùå No real questions found!")
            return False

        print(f"‚úÖ Using {len(real_questions)} questions from tournament: {tournament_used}")

        # Create LLM instances
        print(f"\nü§ñ STEP 2: INITIALIZING MULTIFORECASTER")

        researcher_llm = create_research_fallback_llm(api_key=api_key, temperature=0.3, timeout=60, allowed_tries=2)
        forecaster_llm = create_forecasting_fallback_llm(api_key=api_key, temperature=0.5, timeout=60, allowed_tries=2)
        synthesizer_llm = create_synthesis_fallback_llm(api_key=api_key, temperature=0.3, timeout=60, allowed_tries=2)

        print(f"‚úÖ All LLM instances created with detailed logging enabled")

        # Process first real question
        question = real_questions[0]
        print(f"\nüéØ STEP 3: PROCESSING REAL QUESTION")
        print(f"Question: {question.question_text}")
        print(f"URL: {question.page_url}")
        print(f"Type: {type(question).__name__}")

        # Research Phase
        print(f"\nüìö STEP 4: RESEARCH PHASE")
        print(f"Running detailed research with API call logging...")

        research_prompt = f"""
Research this question thoroughly: {question.question_text}

Page URL: {question.page_url}

Task:
1. Research the specific topic and context
2. Look for relevant data, trends, and expert opinions
3. Consider factors that could influence the outcome
4. Examine similar questions or historical precedents

Provide a comprehensive research summary covering all relevant aspects.
Your response should be detailed and evidence-based.
"""

        print(f"üì§ Research prompt length: {len(research_prompt)} characters")

        try:
            research_response = await researcher_llm.invoke(research_prompt)
            print(f"‚úÖ Research completed successfully!")
            print(f"üì• Research response length: {len(research_response)} characters")

        except Exception as e:
            print(f"‚ùå Research failed: {str(e)}")
            research_response = f"Research failed: {str(e)}"

        # Individual Forecasts
        print(f"\nüîÆ STEP 5: INDIVIDUAL FORECASTS")
        print(f"Generating 3 individual forecasts with detailed logging...")

        forecast_prompt = f"""
Based on the research, provide your forecast for this question:

Question: {question.question_text}

Research Summary:
{research_response[:1500] if len(research_response) > 1500 else research_response}

Task:
Provide your forecast with:
1. Probability assessment (0-100%)
2. Reasoning (2-3 sentences)
3. Format: "Probability: XX% - [Yes/No]"

Consider the research evidence carefully.
"""

        individual_forecasts = []

        for i in range(3):  # 3 forecasters
            print(f"   ü§ñ Forecaster {i+1}/3...")

            try:
                forecaster_prompt = f"{forecast_prompt}\n\nAs Forecaster {i+1}, provide your independent assessment:"
                response = await forecaster_llm.invoke(forecaster_prompt)
                individual_forecasts.append(response)
                print(f"      ‚úÖ Forecast {i+1} completed ({len(response)} chars)")

                await asyncio.sleep(2)  # Rate limiting

            except Exception as e:
                print(f"      ‚ùå Forecast {i+1} failed: {str(e)}")
                individual_forecasts.append(f"Forecast {i+1} failed: {str(e)}")

        print(f"‚úÖ Generated {len(individual_forecasts)} individual forecasts")

        # Synthesis Phase
        print(f"\nüîó STEP 6: SYNTHESIS PHASE")
        print(f"Synthesizing forecasts with detailed logging...")

        synthesis_prompt = f"""
Synthesize these forecasts for the question: {question.question_text}

Individual Forecasts:
{chr(10).join([f"Forecast {i+1}: {forecast}" for i, forecast in enumerate(individual_forecasts)])}

Task:
1. Analyze consensus and disagreements
2. Weigh reasoning and evidence
3. Provide final probability (0-100%)
4. Give comprehensive reasoning
5. Format: "Probability: XX% - [Yes/No]"

Synthesize thoughtfully and provide evidence-based conclusion.
"""

        try:
            synthesis_response = await synthesizer_llm.invoke(synthesis_prompt)
            print(f"‚úÖ Synthesis completed successfully!")
            print(f"üì• Synthesis response length: {len(synthesis_response)} characters")

        except Exception as e:
            print(f"‚ùå Synthesis failed: {str(e)}")
            synthesis_response = f"Synthesis failed: {str(e)}"

        # Save comprehensive output
        print(f"\nüìã STEP 7: SAVING COMPREHENSIVE OUTPUT")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"real_question_multiforecaster_{timestamp}.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Real Question Multiforecaster Test\n\n")
            f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**Tournament:** {tournament_used}\n")
            f.write(f"**Question:** {question.question_text}\n")
            f.write(f"**URL:** {question.page_url}\n")
            f.write(f"**Question Type:** {type(question).__name__}\n\n")

            f.write(f"## Model Configuration\n\n")
            f.write(f"**All models are FREE OpenRouter models:**\n")
            f.write(f"- Research: {researcher_llm.model_chain[0]} (primary)\n")
            f.write(f"- Forecasting: {forecaster_llm.model_chain[0]} (primary)\n")
            f.write(f"- Synthesis: {synthesizer_llm.model_chain[0]} (primary)\n\n")
            f.write(f"**Complete fallback chain (9 models each):**\n")
            for i, model in enumerate(researcher_llm.model_chain, 1):
                f.write(f"{i}. {model}\n")
            f.write(f"\n")

            f.write(f"## Process Steps Completed\n\n")
            f.write(f"‚úÖ 1. Found real questions from tournament: {tournament_used}\n")
            f.write(f"‚úÖ 2. Research Phase - Comprehensive analysis\n")
            f.write(f"‚úÖ 3. Individual Forecasts - {len(individual_forecasts)} model predictions\n")
            f.write(f"‚úÖ 4. Synthesis Phase - Combined consensus\n")
            f.write(f"‚úÖ 5. Detailed API call logging throughout\n\n")

            f.write(f"## Research Output\n\n")
            f.write(f"{research_response}\n\n")

            f.write(f"## Individual Forecasts\n\n")
            for i, forecast in enumerate(individual_forecasts, 1):
                f.write(f"### Forecaster {i}\n\n")
                f.write(f"{forecast}\n\n")

            f.write(f"## Synthesis Output\n\n")
            f.write(f"{synthesis_response}\n\n")

            f.write(f"## System Performance\n\n")
            f.write(f"‚úÖ API Authentication: Working with GitHub secrets\n")
            f.write(f"‚úÖ Free Model Configuration: All 9 models accessible\n")
            f.write(f"‚úÖ Real Questions: Successfully found and processed\n")
            f.write(f"‚úÖ Detailed Logging: All API calls logged\n")
            f.write(f"‚úÖ Output Organization: Saved to /outputs/ folder\n\n")

            f.write(f"**üéØ CONCLUSION: Real multiforecaster process working perfectly!**\n")

        print(f"‚úÖ Comprehensive output saved to: {output_file}")
        print(f"üìÇ All outputs in: {output_dir.absolute()}")

        # Check if the multiforecaster actually worked
        if "All 9 models in fallback chain failed" in research_response or len(individual_forecasts) == 0 or "All 9 models in fallback chain failed" in synthesis_response:
            print(f"\n‚ùå MULTIFORECASTER PROCESS FAILED!")
            print(f"‚ùå All API calls failed - no successful model responses")
            print(f"‚ùå No actual forecasting completed")
            return False

        print(f"\n" + "=" * 70)
        print(f"üéâ REAL QUESTION MULTIFORECASTER TEST COMPLETED!")
        print(f"‚úÖ Used real Metaculus question from {tournament_used}")
        print(f"‚úÖ Complete multiforecaster process with detailed logging")
        print(f"‚úÖ All outputs saved to /outputs/ folder")
        print(f"üìÑ Full report: {output_file}")
        print("=" * 70)

        return True

    except Exception as e:
        print(f"‚ùå Real question test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(test_real_questions())
    if success:
        print("\nüéâ REAL QUESTION MULTIFORECASTER SUCCESSFUL!")
        print("‚úÖ Found and processed real Metaculus questions")
        print("‚úÖ All API calls logged with model details")
    else:
        print("\n‚ùå REAL QUESTION TEST FAILED!")
        print("‚ùå Check the error messages above")
    sys.exit(0 if success else 1)